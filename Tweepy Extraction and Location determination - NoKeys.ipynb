{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Important informations\n",
    "StatesFull = ['Alabama','Alaska','Arizona','Arkansas','California', 'Colorado','Connecticut',\n",
    "          'Delaware','Florida','Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa',\n",
    "          'Kansas','Kentucky','Louisiana','Maine','Maryland','Massachusetts','Michigan','Minnesota',\n",
    "          'Mississippi','Missouri','Montana','Nebraska','Nevada','New Hampshire','New Jersey',\n",
    "          'New Mexico','New York','North Carolina','North Dakota','Ohio','Oklahoma','Oregon',\n",
    "          'Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "          'Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming']\n",
    "StatesFull=[S.lower() for S in StatesFull]\n",
    "print(len(StatesFull))\n",
    "StatesTL=['AL','AK','AZ','AR','CA','CO','CT','DE','FL','KY','LA','ME','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK',\n",
    "         'OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY','KS','GA','HI','MD','MA','ID','IL','IN','MI',\n",
    "          'IA','MN','MS','MO']\n",
    "StatesTL=[S.lower() for S in StatesTL]\n",
    "print(len(StatesTL))\n",
    "States_dict={'Alabama' : 'AL','Alaska' : 'AK','Arizona' : 'AZ','Arkansas' : 'AR','California': 'CA','Colorado' : 'CO',\n",
    "             'Connecticut' : 'CT','Delaware' : 'DE','Florida' : 'FL','Georgia' : 'GA','Hawaii' : 'HI','Idaho' : 'ID',\n",
    "             'Illinois' : 'IL','Indiana' : 'IN','Iowa' : 'IA', 'Kansas' : 'KS','Kentucky' : 'KY','Louisiana' : 'LA','Maine':'ME',\n",
    "             'Maryland' : 'MD','Massachusetts': 'MA' ,'Michigan' : 'MI','Minnesota' : 'MN', 'Mississippi' : 'MS','Missouri' : 'MO',\n",
    "             'Montana' : 'MT','Nebraska' : 'NE','Nevada' : 'NV','New hampshire' : 'NH','New jersey' : 'NJ','New mexico' : 'NM',\n",
    "             'New york' : 'NY', 'North carolina' : 'NC','North dakota' : 'ND', 'Ohio' : 'OH', 'Oklahoma' : 'OK','Oregon' : 'OR',\n",
    "             'Pennsylvania' : 'PA', 'Rhode island' : 'RI','South carolina' : 'SC','South dakota' : 'SD','Tennessee' : 'TN',\n",
    "             'Texas' : 'TX', 'Utah' : 'UT', 'Vermont' : 'VT', 'Virginia' : 'VA','Washington' : 'WA', 'West virginia' : 'WV',\n",
    "             'Wisconsin' :'WI', 'Wyoming':'WY'}\n",
    "print(len(States_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your twitter credentials\n",
    "\n",
    "twitter_cred = dict()\n",
    "\n",
    "# Enter your own consumer_key, consumer_secret, access_key and access_secret\n",
    "# Replacing the stars (\"********\")\n",
    "\n",
    "twitter_cred['CONSUMER_KEY'] = ''\n",
    "twitter_cred['CONSUMER_SECRET'] = ''\n",
    "twitter_cred['ACCESS_KEY'] = ''\n",
    "twitter_cred['ACCESS_SECRET'] = ''\n",
    "\n",
    "# Save the information to a json so that it can be reused in code without exposing\n",
    "# the secret info to public\n",
    "\n",
    "with open('twitter_credentials.json', 'w') as secret_info:\n",
    "    json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Twitter API credentials\n",
    "with open('twitter_credentials.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_tweets(query,limit=None):\n",
    "        \"\"\"\n",
    "        returns all the tweets within 7 days top according to the query received by this method\n",
    "        returns the complete tweet\n",
    "        :param query: should contain all the words and can include logic operators\n",
    "        should also provide the period of time for the search\n",
    "        ex: rock OR axe \n",
    "        (visit https://dev.twitter.com/rest/public/search to see how to create a query)\n",
    "        :param lang: the language of the tweets\n",
    "        :param limit: defines the maximum amount of tweets to fetch\n",
    "        :return: tweets: a list of all tweets obtained after the request\n",
    "        \"\"\"\n",
    "        tweets = []\n",
    "\n",
    "        for tweet in tweepy.Cursor(api.search, q=query).items(limit):\n",
    "            JSON=tweet._json\n",
    "            JSON['query'] = query\n",
    "            tweets.append(JSON)\n",
    "            #time.(delay)\n",
    "\n",
    "        return tweets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(tweets):\n",
    "    df = pd.DataFrame(columns=['Query','TweetId','UserId','ScreenName', 'Date', 'Location', 'Coordinates', 'Place', 'Text'])\n",
    "    i=0\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            \n",
    "            d = [{'Query': tweet['query'],'TweetId': tweet['id'],'UserId':tweet['user']['id'],'ScreenName':tweet['user']['screen_name'], \n",
    "                   'Date':tweet['created_at'], 'Location':tweet['user']['location'], 'Coordinates':tweet['coordinates'],\n",
    "                   'Place':tweet['place'], 'Text':tweet['text']}]\n",
    "            newrow= pd.DataFrame(d)\n",
    "\n",
    "            df=df.append(newrow, ignore_index=True)\n",
    "            i=i+1\n",
    "        except:\n",
    "            print(i, 'exception')\n",
    "            i=i+1\n",
    "\n",
    "   \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit the content of the location column using a co\n",
    "# Check if the Location contains the two-letter state code \n",
    "USNames=['united states', 'usa', 'us']\n",
    "def determine_USState(all_tweets):\n",
    "    StatesTest=[]\n",
    "    for i,L in enumerate(all_tweets['Location']):\n",
    "        L=L.strip().lower()\n",
    "        if ',' in L: ## Orlando, FL\n",
    "            test=L.split(',')[1] \n",
    "            test=test.strip()\n",
    "            if test in StatesTL:\n",
    "                StatesTest.append(test.capitalize())\n",
    "            elif test in StatesFull:\n",
    "                StatesTest.append(States_dict[test.capitalize()])\n",
    "            elif test in USNames: # For these cases Florida, USA\n",
    "                firstelement=L.split(',')[0]\n",
    "                if firstelement in StatesTL:\n",
    "                    StatesTest.append(firstelement.capitalize())\n",
    "                elif firstelement in StatesFull:\n",
    "                    StatesTest.append(States_dict[firstelement.capitalize()])\n",
    "                else:\n",
    "                    StatesTest.append('probably not US')\n",
    "                    next  \n",
    "            else:\n",
    "                StatesTest.append('probably not US')\n",
    "                next           \n",
    "        elif L in StatesFull:\n",
    "            StatesTest.append(States_dict[L.capitalize()])\n",
    "        else:\n",
    "            StatesTest.append('probably not US')\n",
    "\n",
    "    all_tweets['State']=StatesTest\n",
    "    all_tweets['State']=all_tweets['State'].str.lower()\n",
    "    return(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurants\n",
    "## Healthy restaurant chains (15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some data 1000 tweets naming one of those restaurants\n",
    "test=['Panerabread']\n",
    "healthyRestaurants=['Panerabread','jasonsdeli','aubonpain','Noodles and Company','Chipotle','AtlantaBread', 'EinsteinBros',\n",
    "                   'LePainQuotidien', 'Justsalad', 'Mymarthas','krunch','chopt', 'sweetgreen', 'cava','olivegarden']\n",
    "print('Number of healthy Restaurants tested:',len(healthyRestaurants))\n",
    "healthy_tweets_list=[]\n",
    "for Rest in healthyRestaurants:\n",
    "    healthy_tweets=scroll_tweets(query=Rest, limit=2000)\n",
    "    healthy_tweets_list.append(healthy_tweets)\n",
    "    \n",
    "hRest_t_l = [item for sublist in healthy_tweets_list for item in sublist]\n",
    "len(hRest_t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's locate those tweets\n",
    "healthy_tweet_df=extract_information(hRest_t_l)\n",
    "healthy_tweet_df['Cat']='healthy'\n",
    "healthy_tweet_df=determine_USState(healthy_tweet_df)\n",
    "healthy_tweet_df.to_csv('healthyRestaurants_tweet_df_NEW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unhealthy restaurant chains (19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthyRestaurants=['kfc','tacobell','burgerking','cinnabon','chickfila','PandaExpress','dunkindonuts','pizzahut','WaffleHouse',\n",
    "                     'cinnabon','AutieAnnes','cheesecake','arbys','wendys', 'FiveGuys','Shakeshack','WhiteCastle','DairyQueen',\n",
    "                     'quiznos']\n",
    "print('Number of unhealthy Restaurants tested:',len(unhealthyRestaurants))\n",
    "unhealthy_tweets_list=[]\n",
    "for Rest in unhealthyRestaurants:\n",
    "    unhealthy_tweets=scroll_tweets(query=Rest, limit=2000)\n",
    "    unhealthy_tweets_list.append(unhealthy_tweets)\n",
    "uhRest_t_l = [item for sublist in unhealthy_tweets_list for item in sublist]\n",
    "len(uhRest_t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uhRest_t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's locate those tweets\n",
    "unhealthy_tweet_df=extract_information(uhRest_t_l)\n",
    "unhealthy_tweet_df['Cat']='unhealthy'\n",
    "unhealthy_tweet_df=determine_USState(unhealthy_tweet_df)\n",
    "unhealthy_tweet_df.to_csv('unhealthyResturants_tweet_df_NEW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activities\n",
    "## Healthy activities (44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthyActivities=['run','running','ran','walk','walking','walked','hike','hiking','hiked','surf','surfing','yoga','exercise',\n",
    "                  'climb','climbed','soccer','tennis','volleyball','baseball','softball','swim','swimming','swam','dance','ballet',\n",
    "                  'mountain bike','marathon','triathlon','boxing','kick boxing','gymnastics','ski','skiing','snow boarding', 'snowboard',\n",
    "                   'kanu','kayak','row','rowing','sail','sailing','sailed', 'body building', 'spinning', 'cardio']\n",
    "print('Number of healthy Activities tested:',len(healthyActivities))\n",
    "healthy_acts_list=[]\n",
    "for act in healthyActivities:\n",
    "    healthyact_tweets=scroll_tweets(query=act, limit=2000)\n",
    "    healthy_acts_list.append(healthyact_tweets)\n",
    "hAct_t_l = [item for sublist in healthy_acts_list for item in sublist]\n",
    "len(hAct_t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthyact_tweet_df=extract_information(hAct_t_l)\n",
    "healthyact_tweet_df['Cat']='healthy'\n",
    "healthyact_tweet_df=determine_USState(healthyact_tweet_df)\n",
    "healthyact_tweet_df.to_csv('healthyACT_tweet_df_NEW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unhealthy Activities (38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthyActivities=['couch','sofa','nap','sleep','TV','watch','watching','watched','HBO','Netflix','binge watch','binge watched','HULU',\n",
    "                  'Amazon Video','season','prime video','television','slinc','CBS','philo','fuboTV','direct TV','Youtube TV','Youtube','playstation',\n",
    "                  'xbox','wii','ESPN','Showtime','ABC','Starz','Fox','binge','Pluto TV', 'lazy','cozy','blanket','pillow']\n",
    "print('Number of unhealthy activities tested:',len(unhealthyActivities))\n",
    "unhealthy_acts_list=[]\n",
    "for act in unhealthyActivities:\n",
    "    unhealthyact_tweets=scroll_tweets(query=act, limit=2000)\n",
    "    unhealthy_acts_list.append(unhealthyact_tweets)\n",
    "uha_t_l = [item for sublist in unhealthy_acts_list for item in sublist]\n",
    "len(uha_t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's locate those tweets\n",
    "unhealthyact_tweet_df=extract_information(uha_t_l)\n",
    "unhealthyact_tweet_df['Cat']='unhealthy'\n",
    "unhealthyact_tweet_df=determine_USState(unhealthyact_tweet_df)\n",
    "unhealthyact_tweet_df.to_csv('unhealthyACT_tweet_df_NEW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foods\n",
    "## Fruits and veggies (66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "healthyFooddict={'healthyFoods2':['banana','blackberries','blueberries','cherry','coconut','cranberry','date','fig','goji'],\n",
    "                'healthyFoods3':['grape','grapefruit','kiwi','lemon','lime','lyche','mango','melon','watermelon','nectarine'],\n",
    "                'healthyFoods4':['orange','papaya','passionfruit','peach','pear','plum','pineapple','pomegranate','raspberry'],\n",
    "                'healthyFoods5':['star fruit','strawberry','cantaloupe','artichoke','asparagus','beans','legumes','broccoli'],\n",
    "                'healthyFoods6':['brussels sprouts','cabbage','cauliflower','celery','endives','fennel','kale','spinach'],\n",
    "                'healthyFoods7':['lettuce','salad','mushrooms','okra','garlic','chives',\n",
    "             'beetroot','beets','ginger','radish','squash','tomato']}\n",
    "\n",
    "healthy_food_list=[]\n",
    "for k,v in healthyFooddict.items():\n",
    "    print(k)\n",
    "    print(len(v))\n",
    "    for food in v:\n",
    "        try:\n",
    "            healthyfood_tweets=scroll_tweets(query=food, limit=2000)\n",
    "            healthy_food_list.append(healthyfood_tweets)\n",
    "        except requests.Timeout as err:\n",
    "             print(food, err.message)\n",
    "    #flatten the tweet list   \n",
    "    hf_t_l = [item for sublist in healthy_food_list for item in sublist]\n",
    "    print('Length of tweet list from', k,':', len(hf_t_l))\n",
    "    # Let's locate those tweets\n",
    "    healthyfood_tweet_df=extract_information(hf_t_l)\n",
    "    healthyfood_tweet_df['Cat']='healthy'\n",
    "    healthyfood_tweet_df=determine_USState(healthyfood_tweet_df)\n",
    "    filename=k+'_tweet_df_NEW.csv'\n",
    "    healthyfood_tweet_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unhealthy Foods (28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnhealthyFoodDict={'unhealthyFood1':['lemonade','coke','soda','sprite','pepsi','pizza','fries','burger'],\n",
    "                    'unhealthyFood2':['cheeseburger','cheese','cream','sauce','cupcake','cake','cookie'],\n",
    "                   'unhealthyFood3':['donut','chips','syrup','candy','fudge','pie','pudding','brownie'],\n",
    "                   'unhealthyFood4':['all you can eat','frozen yoghurt','chicken nuggets','waffle','pancake']}\n",
    "unhealthy_food_list=[]\n",
    "for k,v in UnhealthyFoodDict.items():\n",
    "    print(k)\n",
    "    print(len(v))\n",
    "    for food in v:\n",
    "        print(food)\n",
    "        try:\n",
    "            unhealthyfood_tweets=scroll_tweets(query=food, limit=2000)\n",
    "            unhealthy_food_list.append(unhealthyfood_tweets)\n",
    "        except requests.Timeout as err:\n",
    "             print(food, err.message)\n",
    "    #flatten the tweet list   \n",
    "    uhf_t_l = [item for sublist in unhealthy_food_list for item in sublist]\n",
    "    print('Length of tweet list from', k,':', len(uhf_t_l))\n",
    "    # Let's locate those tweets\n",
    "    unhealthyfood_tweet_df=extract_information(uhf_t_l)\n",
    "    unhealthyfood_tweet_df['Cat']='unhealthy'\n",
    "    unhealthyfood_tweet_df=determine_USState(unhealthyfood_tweet_df)\n",
    "    filename=k+'_tweet_df_NEW.csv'\n",
    "    unhealthyfood_tweet_df.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
