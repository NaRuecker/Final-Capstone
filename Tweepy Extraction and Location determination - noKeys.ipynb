{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim\n",
    "Read all csv files generated by the twint analysis. Twint analysis did only return, tweets but not user location. Do get user location, read the csv and query each user and extract the location. <br>\n",
    "Once the location has been extracted, the location has to be mapped to a US State.\n",
    "\n",
    "# Important informations for States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Important informations\n",
    "StatesFull = ['Alabama','Alaska','Arizona','Arkansas','California', 'Colorado','Connecticut',\n",
    "          'Delaware','Florida','Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa',\n",
    "          'Kansas','Kentucky','Louisiana','Maine','Maryland','Massachusetts','Michigan','Minnesota',\n",
    "          'Mississippi','Missouri','Montana','Nebraska','Nevada','New Hampshire','New Jersey',\n",
    "          'New Mexico','New York','North Carolina','North Dakota','Ohio','Oklahoma','Oregon',\n",
    "          'Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "          'Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming']\n",
    "StatesFull=[S.lower() for S in StatesFull]\n",
    "print(len(StatesFull))\n",
    "StatesTL=['AL','AK','AZ','AR','CA','CO','CT','DE','FL','KY','LA','ME','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK',\n",
    "         'OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY','KS','GA','HI','MD','MA','ID','IL','IN','MI',\n",
    "          'IA','MN','MS','MO']\n",
    "StatesTL=[S.lower() for S in StatesTL]\n",
    "print(len(StatesTL))\n",
    "States_dict={'Alabama' : 'AL','Alaska' : 'AK','Arizona' : 'AZ','Arkansas' : 'AR','California': 'CA','Colorado' : 'CO',\n",
    "             'Connecticut' : 'CT','Delaware' : 'DE','Florida' : 'FL','Georgia' : 'GA','Hawaii' : 'HI','Idaho' : 'ID',\n",
    "             'Illinois' : 'IL','Indiana' : 'IN','Iowa' : 'IA', 'Kansas' : 'KS','Kentucky' : 'KY','Louisiana' : 'LA','Maine':'ME',\n",
    "             'Maryland' : 'MD','Massachusetts': 'MA' ,'Michigan' : 'MI','Minnesota' : 'MN', 'Mississippi' : 'MS','Missouri' : 'MO',\n",
    "             'Montana' : 'MT','Nebraska' : 'NE','Nevada' : 'NV','New hampshire' : 'NH','New jersey' : 'NJ','New mexico' : 'NM',\n",
    "             'New york' : 'NY', 'North carolina' : 'NC','North dakota' : 'ND', 'Ohio' : 'OH', 'Oklahoma' : 'OK','Oregon' : 'OR',\n",
    "             'Pennsylvania' : 'PA', 'Rhode island' : 'RI','South carolina' : 'SC','South dakota' : 'SD','Tennessee' : 'TN',\n",
    "             'Texas' : 'TX', 'Utah' : 'UT', 'Vermont' : 'VT', 'Virginia' : 'VA','Washington' : 'WA', 'West virginia' : 'WV',\n",
    "             'Wisconsin' :'WI', 'Wyoming':'WY'}\n",
    "print(len(States_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your twitter credentials\n",
    "\n",
    "twitter_cred = dict()\n",
    "\n",
    "# Enter your own consumer_key, consumer_secret, access_key and access_secret\n",
    "# Replacing the stars (\"********\")\n",
    "\n",
    "twitter_cred['CONSUMER_KEY'] = ''\n",
    "twitter_cred['CONSUMER_SECRET'] = ''\n",
    "twitter_cred['ACCESS_KEY'] = ''\n",
    "twitter_cred['ACCESS_SECRET'] = ''\n",
    "\n",
    "# Save the information to a json so that it can be reused in code without exposing\n",
    "# the secret info to public\n",
    "\n",
    "with open('twitter_credentials.json', 'w') as secret_info:\n",
    "    json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Twitter API credentials\n",
    "with open('twitter_credentials.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit the content of the location column using a co\n",
    "# Check if the Location contains the two-letter state code \n",
    "USNames=['united states', 'usa', 'us']\n",
    "def determine_USState(all_tweets):\n",
    "    StatesTest=[]\n",
    "    for i,L in enumerate(all_tweets['Location']):\n",
    "        L=L.strip().lower()\n",
    "        if ',' in L: ## Orlando, FL\n",
    "            test=L.split(',')[1] \n",
    "            test=test.strip()\n",
    "            if test in StatesTL:\n",
    "                StatesTest.append(test.capitalize())\n",
    "            elif test in StatesFull:\n",
    "                StatesTest.append(States_dict[test.capitalize()])\n",
    "            elif test in USNames: # For these cases Florida, USA\n",
    "                firstelement=L.split(',')[0]\n",
    "                if firstelement in StatesTL:\n",
    "                    StatesTest.append(firstelement.capitalize())\n",
    "                elif firstelement in StatesFull:\n",
    "                    StatesTest.append(States_dict[firstelement.capitalize()])\n",
    "                else:\n",
    "                    StatesTest.append('probably not US')\n",
    "                    next  \n",
    "            else:\n",
    "                StatesTest.append('probably not US')\n",
    "                next           \n",
    "        elif L in StatesFull:\n",
    "            StatesTest.append(States_dict[L.capitalize()])\n",
    "        else:\n",
    "            StatesTest.append('probably not US')\n",
    "\n",
    "    all_tweets['State']=StatesTest\n",
    "    all_tweets['State']=all_tweets['State'].str.lower()\n",
    "    return(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each individual csv and \n",
    "import glob\n",
    "path = \"Foods2016\\*.csv\"\n",
    "for fname in glob.glob(path):\n",
    "    print(fname)\n",
    "    CSV_name=fname.split('\\\\')[1]\n",
    "    Cat=CSV_name.split('_')[0]\n",
    "    Query=CSV_name.split('_')[1]\n",
    "    Year=CSV_name.split('_')[2].split('-')[0]\n",
    "    \n",
    "    f= pd.read_csv(fname)\n",
    "    usernames=f.username\n",
    "    userlocations=[]\n",
    "    for user in usernames:\n",
    "        try:\n",
    "            userdata=api.get_user(user)\n",
    "            userlocations.append(userdata.location)\n",
    "        except: \n",
    "            print(user)\n",
    "            #print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            userlocations.append('')\n",
    "    f['Cat']=Cat\n",
    "    f['Query']=Query\n",
    "    f['Year']=Year\n",
    "    f['Location']=userlocations\n",
    "    f=determine_USState(f)\n",
    "    \n",
    "    newfilename='WithLocation2016\\Loc_'+CSV_name\n",
    "    f.to_csv(newfilename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
